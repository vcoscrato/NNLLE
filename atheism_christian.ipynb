{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "\n",
    "from nnlocallinear import NLS, LLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)\n",
    "class_names = ['atheism', 'christian']\n",
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(lowercase=False)\n",
    "train_vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "test_vectors = vectorizer.transform(newsgroups_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_features = np.array(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23035"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "        'model__es_give_up_after_nepochs': [20]\n",
    "        , 'model__hidden_size': [100, 250, 500]\n",
    "        , 'model__num_layers': [1, 3, 5]\n",
    "    }\n",
    "\n",
    "comb_parameters = [{\n",
    "        'es_give_up_after_nepochs': 20\n",
    "        , 'hidden_size': 20\n",
    "        , 'num_layers': 1\n",
    "        , 'n_classification_labels': 2,\n",
    "\n",
    "    }\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 0 with batch size 300 and train loss 0.6945886015892029\n",
      "Finished epoch 0 with batch size 108 and validation loss 0.6931076645851135\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 1 with batch size 301 and train loss 0.6933244268099467\n",
      "Finished epoch 1 with batch size 108 and validation loss 0.6930197477340698\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 2 with batch size 305 and train loss 0.6926557421684265\n",
      "Finished epoch 2 with batch size 108 and validation loss 0.692654550075531\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 3 with batch size 312 and train loss 0.6895415584246317\n",
      "Finished epoch 3 with batch size 108 and validation loss 0.6921366453170776\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 4 with batch size 322 and train loss 0.6839520931243896\n",
      "Finished epoch 4 with batch size 108 and validation loss 0.6914830207824707\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 5 with batch size 335 and train loss 0.679314911365509\n",
      "Finished epoch 5 with batch size 108 and validation loss 0.6909760236740112\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 6 with batch size 350 and train loss 0.6754706799983978\n",
      "Finished epoch 6 with batch size 108 and validation loss 0.6904129981994629\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 7 with batch size 368 and train loss 0.6702812016010284\n",
      "Finished epoch 7 with batch size 108 and validation loss 0.6897884607315063\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 8 with batch size 389 and train loss 0.666049987077713\n",
      "Finished epoch 8 with batch size 108 and validation loss 0.6890904903411865\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 9 with batch size 413 and train loss 0.6605198979377747\n",
      "Finished epoch 9 with batch size 108 and validation loss 0.6883184909820557\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 10 with batch size 440 and train loss 0.654893547296524\n",
      "Finished epoch 10 with batch size 108 and validation loss 0.687454879283905\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 11 with batch size 469 and train loss 0.64729243516922\n",
      "Finished epoch 11 with batch size 108 and validation loss 0.6864817142486572\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 12 with batch size 501 and train loss 0.6416257619857788\n",
      "Finished epoch 12 with batch size 108 and validation loss 0.6859476566314697\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 13 with batch size 536 and train loss 0.6393797993659973\n",
      "Finished epoch 13 with batch size 108 and validation loss 0.6853779554367065\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 14 with batch size 574 and train loss 0.632420003414154\n",
      "Finished epoch 14 with batch size 108 and validation loss 0.6847659945487976\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 15 with batch size 615 and train loss 0.6307570338249207\n",
      "Finished epoch 15 with batch size 108 and validation loss 0.6841006278991699\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 16 with batch size 658 and train loss 0.6235553622245789\n",
      "Finished epoch 16 with batch size 108 and validation loss 0.683379590511322\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 17 with batch size 704 and train loss 0.6228451132774353\n",
      "Finished epoch 17 with batch size 108 and validation loss 0.6825945973396301\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 18 with batch size 753 and train loss 0.6145638823509216\n",
      "Finished epoch 18 with batch size 108 and validation loss 0.6817370057106018\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 19 with batch size 805 and train loss 0.6098289489746094\n",
      "Finished epoch 19 with batch size 108 and validation loss 0.680796205997467\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 20 with batch size 860 and train loss 0.6048259735107422\n",
      "Finished epoch 20 with batch size 108 and validation loss 0.6797626614570618\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 21 with batch size 917 and train loss 0.5980952978134155\n",
      "Finished epoch 21 with batch size 108 and validation loss 0.6786221861839294\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 22 with batch size 971 and train loss 0.5912314057350159\n",
      "Finished epoch 22 with batch size 108 and validation loss 0.6773597002029419\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 23 with batch size 971 and train loss 0.5853648781776428\n",
      "Finished epoch 23 with batch size 108 and validation loss 0.6759591698646545\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 24 with batch size 971 and train loss 0.5781813859939575\n",
      "Finished epoch 24 with batch size 108 and validation loss 0.6744030117988586\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 25 with batch size 971 and train loss 0.5728877186775208\n",
      "Finished epoch 25 with batch size 108 and validation loss 0.6726715564727783\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 26 with batch size 971 and train loss 0.5636748671531677\n",
      "Finished epoch 26 with batch size 108 and validation loss 0.670742928981781\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 27 with batch size 971 and train loss 0.5563701391220093\n",
      "Finished epoch 27 with batch size 108 and validation loss 0.6685971021652222\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 28 with batch size 971 and train loss 0.5466212034225464\n",
      "Finished epoch 28 with batch size 108 and validation loss 0.666206419467926\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 29 with batch size 971 and train loss 0.541684627532959\n",
      "Finished epoch 29 with batch size 108 and validation loss 0.663544237613678\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 30 with batch size 971 and train loss 0.532195508480072\n",
      "Finished epoch 30 with batch size 108 and validation loss 0.660580039024353\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 31 with batch size 971 and train loss 0.5188009738922119\n",
      "Finished epoch 31 with batch size 108 and validation loss 0.6572932004928589\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 32 with batch size 971 and train loss 0.5129266977310181\n",
      "Finished epoch 32 with batch size 108 and validation loss 0.6536509990692139\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 33 with batch size 971 and train loss 0.5009739995002747\n",
      "Finished epoch 33 with batch size 108 and validation loss 0.6496257781982422\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 34 with batch size 971 and train loss 0.4964450001716614\n",
      "Finished epoch 34 with batch size 108 and validation loss 0.6451868414878845\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 35 with batch size 971 and train loss 0.4826354682445526\n",
      "Finished epoch 35 with batch size 108 and validation loss 0.6403118371963501\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 36 with batch size 971 and train loss 0.4746871888637543\n",
      "Finished epoch 36 with batch size 108 and validation loss 0.6349735856056213\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 37 with batch size 971 and train loss 0.4650222361087799\n",
      "Finished epoch 37 with batch size 108 and validation loss 0.629141092300415\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 38 with batch size 971 and train loss 0.44730669260025024\n",
      "Finished epoch 38 with batch size 108 and validation loss 0.6227994561195374\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 39 with batch size 971 and train loss 0.442352294921875\n",
      "Finished epoch 39 with batch size 108 and validation loss 0.6159276366233826\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 40 with batch size 971 and train loss 0.43101373314857483\n",
      "Finished epoch 40 with batch size 108 and validation loss 0.6085074543952942\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 41 with batch size 971 and train loss 0.4230102300643921\n",
      "Finished epoch 41 with batch size 108 and validation loss 0.6005438566207886\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 42 with batch size 971 and train loss 0.41041579842567444\n",
      "Finished epoch 42 with batch size 108 and validation loss 0.5920223593711853\n",
      "This is the lowest validation loss so far.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 43 with batch size 971 and train loss 0.40235573053359985\n",
      "Finished epoch 43 with batch size 108 and validation loss 0.5829524397850037\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 44 with batch size 971 and train loss 0.3955250084400177\n",
      "Finished epoch 44 with batch size 108 and validation loss 0.5733720660209656\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 45 with batch size 971 and train loss 0.3835945129394531\n",
      "Finished epoch 45 with batch size 108 and validation loss 0.563298761844635\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 46 with batch size 971 and train loss 0.37207287549972534\n",
      "Finished epoch 46 with batch size 108 and validation loss 0.5527699589729309\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 47 with batch size 971 and train loss 0.36144450306892395\n",
      "Finished epoch 47 with batch size 108 and validation loss 0.5418213605880737\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 48 with batch size 971 and train loss 0.35013511776924133\n",
      "Finished epoch 48 with batch size 108 and validation loss 0.5305117964744568\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 49 with batch size 971 and train loss 0.3436054587364197\n",
      "Finished epoch 49 with batch size 108 and validation loss 0.5189075469970703\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 50 with batch size 971 and train loss 0.33256813883781433\n",
      "Finished epoch 50 with batch size 108 and validation loss 0.5070900321006775\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 51 with batch size 971 and train loss 0.3304169476032257\n",
      "Finished epoch 51 with batch size 108 and validation loss 0.4951053857803345\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 52 with batch size 971 and train loss 0.31756511330604553\n",
      "Finished epoch 52 with batch size 108 and validation loss 0.4830348789691925\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 53 with batch size 971 and train loss 0.305606484413147\n",
      "Finished epoch 53 with batch size 108 and validation loss 0.4709621071815491\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 54 with batch size 971 and train loss 0.29511433839797974\n",
      "Finished epoch 54 with batch size 108 and validation loss 0.45896342396736145\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 55 with batch size 971 and train loss 0.2942030429840088\n",
      "Finished epoch 55 with batch size 108 and validation loss 0.447112113237381\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 56 with batch size 971 and train loss 0.2831812798976898\n",
      "Finished epoch 56 with batch size 108 and validation loss 0.4354805648326874\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 57 with batch size 971 and train loss 0.27779632806777954\n",
      "Finished epoch 57 with batch size 108 and validation loss 0.42410773038864136\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 58 with batch size 971 and train loss 0.27304980158805847\n",
      "Finished epoch 58 with batch size 108 and validation loss 0.41304531693458557\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 59 with batch size 971 and train loss 0.25892508029937744\n",
      "Finished epoch 59 with batch size 108 and validation loss 0.4023420512676239\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 60 with batch size 971 and train loss 0.2533721327781677\n",
      "Finished epoch 60 with batch size 108 and validation loss 0.3920080065727234\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 61 with batch size 971 and train loss 0.2508609890937805\n",
      "Finished epoch 61 with batch size 108 and validation loss 0.38207510113716125\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 62 with batch size 971 and train loss 0.24336785078048706\n",
      "Finished epoch 62 with batch size 108 and validation loss 0.3725559115409851\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 63 with batch size 971 and train loss 0.2342679798603058\n",
      "Finished epoch 63 with batch size 108 and validation loss 0.3634493350982666\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 64 with batch size 971 and train loss 0.2251405417919159\n",
      "Finished epoch 64 with batch size 108 and validation loss 0.3547693192958832\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 65 with batch size 971 and train loss 0.219538614153862\n",
      "Finished epoch 65 with batch size 108 and validation loss 0.34649166464805603\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 66 with batch size 971 and train loss 0.21695679426193237\n",
      "Finished epoch 66 with batch size 108 and validation loss 0.33861044049263\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 67 with batch size 971 and train loss 0.20737051963806152\n",
      "Finished epoch 67 with batch size 108 and validation loss 0.33113428950309753\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 68 with batch size 971 and train loss 0.20907016098499298\n",
      "Finished epoch 68 with batch size 108 and validation loss 0.3240192234516144\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 69 with batch size 971 and train loss 0.2001025229692459\n",
      "Finished epoch 69 with batch size 108 and validation loss 0.31731948256492615\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 70 with batch size 971 and train loss 0.19479583203792572\n",
      "Finished epoch 70 with batch size 108 and validation loss 0.310960978269577\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 71 with batch size 971 and train loss 0.18951982259750366\n",
      "Finished epoch 71 with batch size 108 and validation loss 0.3049336075782776\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 72 with batch size 971 and train loss 0.18544521927833557\n",
      "Finished epoch 72 with batch size 108 and validation loss 0.29921799898147583\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 73 with batch size 971 and train loss 0.18095076084136963\n",
      "Finished epoch 73 with batch size 108 and validation loss 0.2938069999217987\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 74 with batch size 971 and train loss 0.1761697232723236\n",
      "Finished epoch 74 with batch size 108 and validation loss 0.288649320602417\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 75 with batch size 971 and train loss 0.17469008266925812\n",
      "Finished epoch 75 with batch size 108 and validation loss 0.28371021151542664\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 76 with batch size 971 and train loss 0.16755861043930054\n",
      "Finished epoch 76 with batch size 108 and validation loss 0.2790273129940033\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 77 with batch size 971 and train loss 0.16789540648460388\n",
      "Finished epoch 77 with batch size 108 and validation loss 0.27455466985702515\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 78 with batch size 971 and train loss 0.15746411681175232\n",
      "Finished epoch 78 with batch size 108 and validation loss 0.27028805017471313\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 79 with batch size 971 and train loss 0.15801280736923218\n",
      "Finished epoch 79 with batch size 108 and validation loss 0.2662107050418854\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 80 with batch size 971 and train loss 0.1487942636013031\n",
      "Finished epoch 80 with batch size 108 and validation loss 0.26234638690948486\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 81 with batch size 971 and train loss 0.1493598222732544\n",
      "Finished epoch 81 with batch size 108 and validation loss 0.2586462199687958\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 82 with batch size 971 and train loss 0.14742636680603027\n",
      "Finished epoch 82 with batch size 108 and validation loss 0.2551054358482361\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 83 with batch size 971 and train loss 0.1394640952348709\n",
      "Finished epoch 83 with batch size 108 and validation loss 0.2516951262950897\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 84 with batch size 971 and train loss 0.1378200203180313\n",
      "Finished epoch 84 with batch size 108 and validation loss 0.24841882288455963\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 85 with batch size 971 and train loss 0.13933585584163666\n",
      "Finished epoch 85 with batch size 108 and validation loss 0.24526312947273254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the lowest validation loss so far.\n",
      "Finished epoch 86 with batch size 971 and train loss 0.13210038840770721\n",
      "Finished epoch 86 with batch size 108 and validation loss 0.24223461747169495\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 87 with batch size 971 and train loss 0.13236552476882935\n",
      "Finished epoch 87 with batch size 108 and validation loss 0.2393151968717575\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 88 with batch size 971 and train loss 0.12921293079853058\n",
      "Finished epoch 88 with batch size 108 and validation loss 0.23650464415550232\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 89 with batch size 971 and train loss 0.12471219897270203\n",
      "Finished epoch 89 with batch size 108 and validation loss 0.233817920088768\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 90 with batch size 971 and train loss 0.12387320399284363\n",
      "Finished epoch 90 with batch size 108 and validation loss 0.23121917247772217\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 91 with batch size 971 and train loss 0.11925496906042099\n",
      "Finished epoch 91 with batch size 108 and validation loss 0.22871148586273193\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 92 with batch size 971 and train loss 0.1170724406838417\n",
      "Finished epoch 92 with batch size 108 and validation loss 0.2262784093618393\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 93 with batch size 971 and train loss 0.11352305114269257\n",
      "Finished epoch 93 with batch size 108 and validation loss 0.22392553091049194\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 94 with batch size 971 and train loss 0.11302520334720612\n",
      "Finished epoch 94 with batch size 108 and validation loss 0.2216595858335495\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 95 with batch size 971 and train loss 0.11180563271045685\n",
      "Finished epoch 95 with batch size 108 and validation loss 0.21945656836032867\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 96 with batch size 971 and train loss 0.11004846543073654\n",
      "Finished epoch 96 with batch size 108 and validation loss 0.21730369329452515\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 97 with batch size 971 and train loss 0.10634533315896988\n",
      "Finished epoch 97 with batch size 108 and validation loss 0.21519114077091217\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 98 with batch size 971 and train loss 0.10211072117090225\n",
      "Finished epoch 98 with batch size 108 and validation loss 0.21315744519233704\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 99 with batch size 971 and train loss 0.10286145657300949\n",
      "Finished epoch 99 with batch size 108 and validation loss 0.21118681132793427\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 100 with batch size 971 and train loss 0.10106164962053299\n",
      "Finished epoch 100 with batch size 108 and validation loss 0.20926111936569214\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 101 with batch size 971 and train loss 0.09624440968036652\n",
      "Finished epoch 101 with batch size 108 and validation loss 0.20734013617038727\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 102 with batch size 971 and train loss 0.09398272633552551\n",
      "Finished epoch 102 with batch size 108 and validation loss 0.2054634839296341\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 103 with batch size 971 and train loss 0.09495215862989426\n",
      "Finished epoch 103 with batch size 108 and validation loss 0.20362478494644165\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 104 with batch size 971 and train loss 0.09286870062351227\n",
      "Finished epoch 104 with batch size 108 and validation loss 0.20183077454566956\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 105 with batch size 971 and train loss 0.08824104070663452\n",
      "Finished epoch 105 with batch size 108 and validation loss 0.2001185566186905\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 106 with batch size 971 and train loss 0.08882030099630356\n",
      "Finished epoch 106 with batch size 108 and validation loss 0.19846588373184204\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 107 with batch size 971 and train loss 0.08790594339370728\n",
      "Finished epoch 107 with batch size 108 and validation loss 0.19688044488430023\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 108 with batch size 971 and train loss 0.08714275807142258\n",
      "Finished epoch 108 with batch size 108 and validation loss 0.19533738493919373\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 109 with batch size 971 and train loss 0.08435936272144318\n",
      "Finished epoch 109 with batch size 108 and validation loss 0.1938246488571167\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 110 with batch size 971 and train loss 0.08126947283744812\n",
      "Finished epoch 110 with batch size 108 and validation loss 0.19233961403369904\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 111 with batch size 971 and train loss 0.08111074566841125\n",
      "Finished epoch 111 with batch size 108 and validation loss 0.19085468351840973\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 112 with batch size 971 and train loss 0.0777498260140419\n",
      "Finished epoch 112 with batch size 108 and validation loss 0.18942143023014069\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 113 with batch size 971 and train loss 0.07877375185489655\n",
      "Finished epoch 113 with batch size 108 and validation loss 0.18800468742847443\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 114 with batch size 971 and train loss 0.08058889955282211\n",
      "Finished epoch 114 with batch size 108 and validation loss 0.1866093873977661\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 115 with batch size 971 and train loss 0.07970330119132996\n",
      "Finished epoch 115 with batch size 108 and validation loss 0.18523789942264557\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 116 with batch size 971 and train loss 0.07775793969631195\n",
      "Finished epoch 116 with batch size 108 and validation loss 0.18390442430973053\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 117 with batch size 971 and train loss 0.07412039488554001\n",
      "Finished epoch 117 with batch size 108 and validation loss 0.18261586129665375\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 118 with batch size 971 and train loss 0.07544556260108948\n",
      "Finished epoch 118 with batch size 108 and validation loss 0.18134519457817078\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 119 with batch size 971 and train loss 0.07264787703752518\n",
      "Finished epoch 119 with batch size 108 and validation loss 0.18012596666812897\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 120 with batch size 971 and train loss 0.06855839490890503\n",
      "Finished epoch 120 with batch size 108 and validation loss 0.1789437085390091\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 121 with batch size 971 and train loss 0.06683236360549927\n",
      "Finished epoch 121 with batch size 108 and validation loss 0.17781496047973633\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 122 with batch size 971 and train loss 0.06912750005722046\n",
      "Finished epoch 122 with batch size 108 and validation loss 0.17674122750759125\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 123 with batch size 971 and train loss 0.07021471858024597\n",
      "Finished epoch 123 with batch size 108 and validation loss 0.1757075935602188\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 124 with batch size 971 and train loss 0.06793355196714401\n",
      "Finished epoch 124 with batch size 108 and validation loss 0.17471645772457123\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 125 with batch size 971 and train loss 0.06426713615655899\n",
      "Finished epoch 125 with batch size 108 and validation loss 0.17373265326023102\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 126 with batch size 971 and train loss 0.062297627329826355\n",
      "Finished epoch 126 with batch size 108 and validation loss 0.17274326086044312\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 127 with batch size 971 and train loss 0.06272298097610474\n",
      "Finished epoch 127 with batch size 108 and validation loss 0.17178770899772644\n",
      "This is the lowest validation loss so far.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 128 with batch size 971 and train loss 0.06166938692331314\n",
      "Finished epoch 128 with batch size 108 and validation loss 0.17081844806671143\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 129 with batch size 971 and train loss 0.059823449701070786\n",
      "Finished epoch 129 with batch size 108 and validation loss 0.16986383497714996\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 130 with batch size 971 and train loss 0.061245981603860855\n",
      "Finished epoch 130 with batch size 108 and validation loss 0.16891585290431976\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 131 with batch size 971 and train loss 0.056700725108385086\n",
      "Finished epoch 131 with batch size 108 and validation loss 0.16793982684612274\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 132 with batch size 971 and train loss 0.058620378375053406\n",
      "Finished epoch 132 with batch size 108 and validation loss 0.16696269810199738\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 133 with batch size 971 and train loss 0.056244101375341415\n",
      "Finished epoch 133 with batch size 108 and validation loss 0.16597625613212585\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 134 with batch size 971 and train loss 0.05417041853070259\n",
      "Finished epoch 134 with batch size 108 and validation loss 0.16496124863624573\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 135 with batch size 971 and train loss 0.05633806437253952\n",
      "Finished epoch 135 with batch size 108 and validation loss 0.16397762298583984\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 136 with batch size 971 and train loss 0.055404387414455414\n",
      "Finished epoch 136 with batch size 108 and validation loss 0.16302278637886047\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 137 with batch size 971 and train loss 0.052296530455350876\n",
      "Finished epoch 137 with batch size 108 and validation loss 0.16206416487693787\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 138 with batch size 971 and train loss 0.05300886556506157\n",
      "Finished epoch 138 with batch size 108 and validation loss 0.16112524271011353\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 139 with batch size 971 and train loss 0.05102583393454552\n",
      "Finished epoch 139 with batch size 108 and validation loss 0.16016462445259094\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 140 with batch size 971 and train loss 0.04870368912816048\n",
      "Finished epoch 140 with batch size 108 and validation loss 0.15922889113426208\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 141 with batch size 971 and train loss 0.05244440957903862\n",
      "Finished epoch 141 with batch size 108 and validation loss 0.15831013023853302\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 142 with batch size 971 and train loss 0.05160565674304962\n",
      "Finished epoch 142 with batch size 108 and validation loss 0.15742145478725433\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 143 with batch size 971 and train loss 0.051699038594961166\n",
      "Finished epoch 143 with batch size 108 and validation loss 0.1564946472644806\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 144 with batch size 971 and train loss 0.049931835383176804\n",
      "Finished epoch 144 with batch size 108 and validation loss 0.1555936485528946\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 145 with batch size 971 and train loss 0.04969961941242218\n",
      "Finished epoch 145 with batch size 108 and validation loss 0.1547389030456543\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 146 with batch size 971 and train loss 0.047867611050605774\n",
      "Finished epoch 146 with batch size 108 and validation loss 0.15391817688941956\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 147 with batch size 971 and train loss 0.04860034957528114\n",
      "Finished epoch 147 with batch size 108 and validation loss 0.15310555696487427\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 148 with batch size 971 and train loss 0.04744146019220352\n",
      "Finished epoch 148 with batch size 108 and validation loss 0.15233169496059418\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 149 with batch size 971 and train loss 0.046926140785217285\n",
      "Finished epoch 149 with batch size 108 and validation loss 0.15157416462898254\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 150 with batch size 971 and train loss 0.047398410737514496\n",
      "Finished epoch 150 with batch size 108 and validation loss 0.15085293352603912\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 151 with batch size 971 and train loss 0.046863656491041183\n",
      "Finished epoch 151 with batch size 108 and validation loss 0.15015429258346558\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 152 with batch size 971 and train loss 0.045605942606925964\n",
      "Finished epoch 152 with batch size 108 and validation loss 0.1494770348072052\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 153 with batch size 971 and train loss 0.043253496289253235\n",
      "Finished epoch 153 with batch size 108 and validation loss 0.1487807035446167\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 154 with batch size 971 and train loss 0.043854065239429474\n",
      "Finished epoch 154 with batch size 108 and validation loss 0.14812137186527252\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 155 with batch size 971 and train loss 0.04386955499649048\n",
      "Finished epoch 155 with batch size 108 and validation loss 0.14746223390102386\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 156 with batch size 971 and train loss 0.043310582637786865\n",
      "Finished epoch 156 with batch size 108 and validation loss 0.1468183398246765\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 157 with batch size 971 and train loss 0.04195711761713028\n",
      "Finished epoch 157 with batch size 108 and validation loss 0.14620338380336761\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 158 with batch size 971 and train loss 0.03942853957414627\n",
      "Finished epoch 158 with batch size 108 and validation loss 0.14558589458465576\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 159 with batch size 971 and train loss 0.04072064906358719\n",
      "Finished epoch 159 with batch size 108 and validation loss 0.14498946070671082\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 160 with batch size 971 and train loss 0.04016634449362755\n",
      "Finished epoch 160 with batch size 108 and validation loss 0.14439339935779572\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 161 with batch size 971 and train loss 0.04183671623468399\n",
      "Finished epoch 161 with batch size 108 and validation loss 0.14385384321212769\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 162 with batch size 971 and train loss 0.03990907967090607\n",
      "Finished epoch 162 with batch size 108 and validation loss 0.14329659938812256\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 163 with batch size 971 and train loss 0.03995795547962189\n",
      "Finished epoch 163 with batch size 108 and validation loss 0.14274698495864868\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 164 with batch size 971 and train loss 0.03692188486456871\n",
      "Finished epoch 164 with batch size 108 and validation loss 0.14221671223640442\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 165 with batch size 971 and train loss 0.03729770705103874\n",
      "Finished epoch 165 with batch size 108 and validation loss 0.1416897177696228\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 166 with batch size 971 and train loss 0.03537489473819733\n",
      "Finished epoch 166 with batch size 108 and validation loss 0.14116235077381134\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 167 with batch size 971 and train loss 0.03834262117743492\n",
      "Finished epoch 167 with batch size 108 and validation loss 0.1406862735748291\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 168 with batch size 971 and train loss 0.037516914308071136\n",
      "Finished epoch 168 with batch size 108 and validation loss 0.14024634659290314\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 169 with batch size 971 and train loss 0.03749166429042816\n",
      "Finished epoch 169 with batch size 108 and validation loss 0.13978804647922516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the lowest validation loss so far.\n",
      "Finished epoch 170 with batch size 971 and train loss 0.03454999625682831\n",
      "Finished epoch 170 with batch size 108 and validation loss 0.1392815113067627\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 171 with batch size 971 and train loss 0.03759147599339485\n",
      "Finished epoch 171 with batch size 108 and validation loss 0.13873285055160522\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 172 with batch size 971 and train loss 0.03292502462863922\n",
      "Finished epoch 172 with batch size 108 and validation loss 0.13818813860416412\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 173 with batch size 971 and train loss 0.03398662060499191\n",
      "Finished epoch 173 with batch size 108 and validation loss 0.13764354586601257\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 174 with batch size 971 and train loss 0.03488216549158096\n",
      "Finished epoch 174 with batch size 108 and validation loss 0.13705869019031525\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 175 with batch size 971 and train loss 0.035153720527887344\n",
      "Finished epoch 175 with batch size 108 and validation loss 0.1364702731370926\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 176 with batch size 971 and train loss 0.034452036023139954\n",
      "Finished epoch 176 with batch size 108 and validation loss 0.13585788011550903\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 177 with batch size 971 and train loss 0.03447275608778\n",
      "Finished epoch 177 with batch size 108 and validation loss 0.13521882891654968\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 178 with batch size 971 and train loss 0.033993981778621674\n",
      "Finished epoch 178 with batch size 108 and validation loss 0.13460570573806763\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 179 with batch size 971 and train loss 0.03345537185668945\n",
      "Finished epoch 179 with batch size 108 and validation loss 0.13400880992412567\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 180 with batch size 971 and train loss 0.032185155898332596\n",
      "Finished epoch 180 with batch size 108 and validation loss 0.133443221449852\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 181 with batch size 971 and train loss 0.03339342400431633\n",
      "Finished epoch 181 with batch size 108 and validation loss 0.13281704485416412\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 182 with batch size 971 and train loss 0.031546927988529205\n",
      "Finished epoch 182 with batch size 108 and validation loss 0.13222157955169678\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 183 with batch size 971 and train loss 0.03166079521179199\n",
      "Finished epoch 183 with batch size 108 and validation loss 0.13164885342121124\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 184 with batch size 971 and train loss 0.03146383911371231\n",
      "Finished epoch 184 with batch size 108 and validation loss 0.13107116520404816\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 185 with batch size 971 and train loss 0.02950548194348812\n",
      "Finished epoch 185 with batch size 108 and validation loss 0.13052058219909668\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 186 with batch size 971 and train loss 0.0282435342669487\n",
      "Finished epoch 186 with batch size 108 and validation loss 0.129997119307518\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 187 with batch size 971 and train loss 0.029683377593755722\n",
      "Finished epoch 187 with batch size 108 and validation loss 0.12950493395328522\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 188 with batch size 971 and train loss 0.029667427763342857\n",
      "Finished epoch 188 with batch size 108 and validation loss 0.1290014535188675\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 189 with batch size 971 and train loss 0.02865251712501049\n",
      "Finished epoch 189 with batch size 108 and validation loss 0.12853342294692993\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 190 with batch size 971 and train loss 0.02905843034386635\n",
      "Finished epoch 190 with batch size 108 and validation loss 0.1281108260154724\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 191 with batch size 971 and train loss 0.0289798304438591\n",
      "Finished epoch 191 with batch size 108 and validation loss 0.12766879796981812\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 192 with batch size 971 and train loss 0.027623843401670456\n",
      "Finished epoch 192 with batch size 108 and validation loss 0.12726737558841705\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 193 with batch size 971 and train loss 0.02888949029147625\n",
      "Finished epoch 193 with batch size 108 and validation loss 0.12693655490875244\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 194 with batch size 971 and train loss 0.02777944877743721\n",
      "Finished epoch 194 with batch size 108 and validation loss 0.12662023305892944\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 195 with batch size 971 and train loss 0.02781820483505726\n",
      "Finished epoch 195 with batch size 108 and validation loss 0.1263083815574646\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 196 with batch size 971 and train loss 0.028877483680844307\n",
      "Finished epoch 196 with batch size 108 and validation loss 0.126006618142128\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 197 with batch size 971 and train loss 0.02686803601682186\n",
      "Finished epoch 197 with batch size 108 and validation loss 0.12572656571865082\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 198 with batch size 971 and train loss 0.02687087468802929\n",
      "Finished epoch 198 with batch size 108 and validation loss 0.1254323571920395\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 199 with batch size 971 and train loss 0.02633432112634182\n",
      "Finished epoch 199 with batch size 108 and validation loss 0.12517942488193512\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 200 with batch size 971 and train loss 0.02613179013133049\n",
      "Finished epoch 200 with batch size 108 and validation loss 0.12487149238586426\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 201 with batch size 971 and train loss 0.025675244629383087\n",
      "Finished epoch 201 with batch size 108 and validation loss 0.12463817000389099\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 202 with batch size 971 and train loss 0.025894317775964737\n",
      "Finished epoch 202 with batch size 108 and validation loss 0.12443556636571884\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 203 with batch size 971 and train loss 0.02463880367577076\n",
      "Finished epoch 203 with batch size 108 and validation loss 0.12422605603933334\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 204 with batch size 971 and train loss 0.026491522789001465\n",
      "Finished epoch 204 with batch size 108 and validation loss 0.1239808052778244\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 205 with batch size 971 and train loss 0.024489542469382286\n",
      "Finished epoch 205 with batch size 108 and validation loss 0.12380005419254303\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 206 with batch size 971 and train loss 0.025235652923583984\n",
      "Finished epoch 206 with batch size 108 and validation loss 0.12362862378358841\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 207 with batch size 971 and train loss 0.026054836809635162\n",
      "Finished epoch 207 with batch size 108 and validation loss 0.12349364161491394\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 208 with batch size 971 and train loss 0.023054007440805435\n",
      "Finished epoch 208 with batch size 108 and validation loss 0.12335389107465744\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 209 with batch size 971 and train loss 0.024181241169571877\n",
      "Finished epoch 209 with batch size 108 and validation loss 0.12318302690982819\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 210 with batch size 971 and train loss 0.023005397990345955\n",
      "Finished epoch 210 with batch size 108 and validation loss 0.12297607958316803\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 211 with batch size 971 and train loss 0.023472607135772705\n",
      "Finished epoch 211 with batch size 108 and validation loss 0.12276022881269455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the lowest validation loss so far.\n",
      "Finished epoch 212 with batch size 971 and train loss 0.023080402985215187\n",
      "Finished epoch 212 with batch size 108 and validation loss 0.12246669083833694\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 213 with batch size 971 and train loss 0.023631691932678223\n",
      "Finished epoch 213 with batch size 108 and validation loss 0.12210340052843094\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 214 with batch size 971 and train loss 0.02293338067829609\n",
      "Finished epoch 214 with batch size 108 and validation loss 0.12171472609043121\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 215 with batch size 971 and train loss 0.02284763567149639\n",
      "Finished epoch 215 with batch size 108 and validation loss 0.12130804359912872\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 216 with batch size 971 and train loss 0.022640585899353027\n",
      "Finished epoch 216 with batch size 108 and validation loss 0.12084922939538956\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 217 with batch size 971 and train loss 0.022465920075774193\n",
      "Finished epoch 217 with batch size 108 and validation loss 0.12040390819311142\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 218 with batch size 971 and train loss 0.02274255082011223\n",
      "Finished epoch 218 with batch size 108 and validation loss 0.11994916200637817\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 219 with batch size 971 and train loss 0.02612951397895813\n",
      "Finished epoch 219 with batch size 108 and validation loss 0.11949591338634491\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 220 with batch size 971 and train loss 0.022665632888674736\n",
      "Finished epoch 220 with batch size 108 and validation loss 0.11900520324707031\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 221 with batch size 971 and train loss 0.022544218227267265\n",
      "Finished epoch 221 with batch size 108 and validation loss 0.11851685494184494\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 222 with batch size 971 and train loss 0.022756202146410942\n",
      "Finished epoch 222 with batch size 108 and validation loss 0.11807090044021606\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 223 with batch size 971 and train loss 0.022306108847260475\n",
      "Finished epoch 223 with batch size 108 and validation loss 0.11765579134225845\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 224 with batch size 971 and train loss 0.022301513701677322\n",
      "Finished epoch 224 with batch size 108 and validation loss 0.11727922409772873\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 225 with batch size 971 and train loss 0.02098342776298523\n",
      "Finished epoch 225 with batch size 108 and validation loss 0.11696014553308487\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 226 with batch size 971 and train loss 0.020666446536779404\n",
      "Finished epoch 226 with batch size 108 and validation loss 0.11661753058433533\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 227 with batch size 971 and train loss 0.021108979359269142\n",
      "Finished epoch 227 with batch size 108 and validation loss 0.11625882238149643\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 228 with batch size 971 and train loss 0.020799245685338974\n",
      "Finished epoch 228 with batch size 108 and validation loss 0.11589303612709045\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 229 with batch size 971 and train loss 0.01983514055609703\n",
      "Finished epoch 229 with batch size 108 and validation loss 0.11552363634109497\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 230 with batch size 971 and train loss 0.02078223042190075\n",
      "Finished epoch 230 with batch size 108 and validation loss 0.1151537373661995\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 231 with batch size 971 and train loss 0.019578907638788223\n",
      "Finished epoch 231 with batch size 108 and validation loss 0.11473442614078522\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 232 with batch size 971 and train loss 0.021340331062674522\n",
      "Finished epoch 232 with batch size 108 and validation loss 0.11439002305269241\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 233 with batch size 971 and train loss 0.019559605047106743\n",
      "Finished epoch 233 with batch size 108 and validation loss 0.1141032800078392\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 234 with batch size 971 and train loss 0.019883345812559128\n",
      "Finished epoch 234 with batch size 108 and validation loss 0.11383216828107834\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 235 with batch size 971 and train loss 0.02019123174250126\n",
      "Finished epoch 235 with batch size 108 and validation loss 0.11353406310081482\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 236 with batch size 971 and train loss 0.01862456649541855\n",
      "Finished epoch 236 with batch size 108 and validation loss 0.11327148228883743\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 237 with batch size 971 and train loss 0.018922606483101845\n",
      "Finished epoch 237 with batch size 108 and validation loss 0.11309129744768143\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 238 with batch size 971 and train loss 0.01852390728890896\n",
      "Finished epoch 238 with batch size 108 and validation loss 0.11290404200553894\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 239 with batch size 971 and train loss 0.019392086192965508\n",
      "Finished epoch 239 with batch size 108 and validation loss 0.11273267865180969\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 240 with batch size 971 and train loss 0.019669899716973305\n",
      "Finished epoch 240 with batch size 108 and validation loss 0.11257758736610413\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 241 with batch size 971 and train loss 0.01819451153278351\n",
      "Finished epoch 241 with batch size 108 and validation loss 0.11249791830778122\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 242 with batch size 971 and train loss 0.018988817930221558\n",
      "Finished epoch 242 with batch size 108 and validation loss 0.11243908852338791\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 243 with batch size 971 and train loss 0.019823890179395676\n",
      "Finished epoch 243 with batch size 108 and validation loss 0.11239161342382431\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 244 with batch size 971 and train loss 0.017090801149606705\n",
      "Finished epoch 244 with batch size 108 and validation loss 0.11237193644046783\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 245 with batch size 971 and train loss 0.01849479414522648\n",
      "Finished epoch 245 with batch size 108 and validation loss 0.11239000409841537\n",
      "Finished epoch 246 with batch size 971 and train loss 0.017982354387640953\n",
      "Finished epoch 246 with batch size 108 and validation loss 0.11245328187942505\n",
      "Finished epoch 247 with batch size 971 and train loss 0.017786692827939987\n",
      "Finished epoch 247 with batch size 108 and validation loss 0.11252399533987045\n",
      "Finished epoch 248 with batch size 971 and train loss 0.018978439271450043\n",
      "Finished epoch 248 with batch size 108 and validation loss 0.11258944123983383\n",
      "Finished epoch 249 with batch size 971 and train loss 0.01775483600795269\n",
      "Finished epoch 249 with batch size 108 and validation loss 0.11259174346923828\n",
      "Finished epoch 250 with batch size 971 and train loss 0.017440030351281166\n",
      "Finished epoch 250 with batch size 108 and validation loss 0.11258307099342346\n",
      "Finished epoch 251 with batch size 971 and train loss 0.01815114915370941\n",
      "Finished epoch 251 with batch size 108 and validation loss 0.11245891451835632\n",
      "Finished epoch 252 with batch size 971 and train loss 0.0175914466381073\n",
      "Finished epoch 252 with batch size 108 and validation loss 0.11232562363147736\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 253 with batch size 971 and train loss 0.0181911401450634\n",
      "Finished epoch 253 with batch size 108 and validation loss 0.1121610552072525\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 254 with batch size 971 and train loss 0.016547387465834618\n",
      "Finished epoch 254 with batch size 108 and validation loss 0.11195246130228043\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 255 with batch size 971 and train loss 0.016577081754803658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 255 with batch size 108 and validation loss 0.11174741387367249\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 256 with batch size 971 and train loss 0.016873689368367195\n",
      "Finished epoch 256 with batch size 108 and validation loss 0.11150237917900085\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 257 with batch size 971 and train loss 0.01678956300020218\n",
      "Finished epoch 257 with batch size 108 and validation loss 0.11116040498018265\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 258 with batch size 971 and train loss 0.016944067552685738\n",
      "Finished epoch 258 with batch size 108 and validation loss 0.11082696169614792\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 259 with batch size 971 and train loss 0.014606528915464878\n",
      "Finished epoch 259 with batch size 108 and validation loss 0.1105155497789383\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 260 with batch size 971 and train loss 0.01765974424779415\n",
      "Finished epoch 260 with batch size 108 and validation loss 0.11017657816410065\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 261 with batch size 971 and train loss 0.016009116545319557\n",
      "Finished epoch 261 with batch size 108 and validation loss 0.10982018709182739\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 262 with batch size 971 and train loss 0.017186809331178665\n",
      "Finished epoch 262 with batch size 108 and validation loss 0.10949385911226273\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 263 with batch size 971 and train loss 0.01618269458413124\n",
      "Finished epoch 263 with batch size 108 and validation loss 0.10920644551515579\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 264 with batch size 971 and train loss 0.014493866823613644\n",
      "Finished epoch 264 with batch size 108 and validation loss 0.10889454931020737\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 265 with batch size 971 and train loss 0.016788003966212273\n",
      "Finished epoch 265 with batch size 108 and validation loss 0.10862147063016891\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 266 with batch size 971 and train loss 0.016784729436039925\n",
      "Finished epoch 266 with batch size 108 and validation loss 0.10838019102811813\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 267 with batch size 971 and train loss 0.015175694599747658\n",
      "Finished epoch 267 with batch size 108 and validation loss 0.10816789418458939\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 268 with batch size 971 and train loss 0.015731310471892357\n",
      "Finished epoch 268 with batch size 108 and validation loss 0.10796809941530228\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 269 with batch size 971 and train loss 0.015775714069604874\n",
      "Finished epoch 269 with batch size 108 and validation loss 0.10775528103113174\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 270 with batch size 971 and train loss 0.014984006993472576\n",
      "Finished epoch 270 with batch size 108 and validation loss 0.1075468584895134\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 271 with batch size 971 and train loss 0.015879599377512932\n",
      "Finished epoch 271 with batch size 108 and validation loss 0.10735858976840973\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 272 with batch size 971 and train loss 0.014033585786819458\n",
      "Finished epoch 272 with batch size 108 and validation loss 0.1072024330496788\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 273 with batch size 971 and train loss 0.014771001413464546\n",
      "Finished epoch 273 with batch size 108 and validation loss 0.10706038773059845\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 274 with batch size 971 and train loss 0.015153625048696995\n",
      "Finished epoch 274 with batch size 108 and validation loss 0.1069662943482399\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 275 with batch size 971 and train loss 0.013181234709918499\n",
      "Finished epoch 275 with batch size 108 and validation loss 0.10690253227949142\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 276 with batch size 971 and train loss 0.014471670612692833\n",
      "Finished epoch 276 with batch size 108 and validation loss 0.10685581713914871\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 277 with batch size 971 and train loss 0.0140980901196599\n",
      "Finished epoch 277 with batch size 108 and validation loss 0.1067422404885292\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 278 with batch size 971 and train loss 0.014188049361109734\n",
      "Finished epoch 278 with batch size 108 and validation loss 0.10665331780910492\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 279 with batch size 971 and train loss 0.014642740599811077\n",
      "Finished epoch 279 with batch size 108 and validation loss 0.1065363809466362\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 280 with batch size 971 and train loss 0.014447816647589207\n",
      "Finished epoch 280 with batch size 108 and validation loss 0.10643568634986877\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 281 with batch size 971 and train loss 0.01358057465404272\n",
      "Finished epoch 281 with batch size 108 and validation loss 0.10631690919399261\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 282 with batch size 971 and train loss 0.01351119764149189\n",
      "Finished epoch 282 with batch size 108 and validation loss 0.10624212771654129\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 283 with batch size 971 and train loss 0.016681186854839325\n",
      "Finished epoch 283 with batch size 108 and validation loss 0.10620704293251038\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 284 with batch size 971 and train loss 0.013955606147646904\n",
      "Finished epoch 284 with batch size 108 and validation loss 0.10615777224302292\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 285 with batch size 971 and train loss 0.013655978254973888\n",
      "Finished epoch 285 with batch size 108 and validation loss 0.10607065260410309\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 286 with batch size 971 and train loss 0.013231636956334114\n",
      "Finished epoch 286 with batch size 108 and validation loss 0.10598655790090561\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 287 with batch size 971 and train loss 0.01237685140222311\n",
      "Finished epoch 287 with batch size 108 and validation loss 0.10582952946424484\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 288 with batch size 971 and train loss 0.014753543771803379\n",
      "Finished epoch 288 with batch size 108 and validation loss 0.10564952343702316\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 289 with batch size 971 and train loss 0.013450277037918568\n",
      "Finished epoch 289 with batch size 108 and validation loss 0.1053958535194397\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 290 with batch size 971 and train loss 0.013213349506258965\n",
      "Finished epoch 290 with batch size 108 and validation loss 0.10520178079605103\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 291 with batch size 971 and train loss 0.013464221730828285\n",
      "Finished epoch 291 with batch size 108 and validation loss 0.10492316633462906\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 292 with batch size 971 and train loss 0.01360827311873436\n",
      "Finished epoch 292 with batch size 108 and validation loss 0.10459957271814346\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 293 with batch size 971 and train loss 0.013409776613116264\n",
      "Finished epoch 293 with batch size 108 and validation loss 0.1042431965470314\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 294 with batch size 971 and train loss 0.012831644155085087\n",
      "Finished epoch 294 with batch size 108 and validation loss 0.10390554368495941\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 295 with batch size 971 and train loss 0.012482622638344765\n",
      "Finished epoch 295 with batch size 108 and validation loss 0.10361290723085403\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 296 with batch size 971 and train loss 0.01357683539390564\n",
      "Finished epoch 296 with batch size 108 and validation loss 0.10342051088809967\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 297 with batch size 971 and train loss 0.012950926087796688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 297 with batch size 108 and validation loss 0.10322780907154083\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 298 with batch size 971 and train loss 0.01222264301031828\n",
      "Finished epoch 298 with batch size 108 and validation loss 0.10309863835573196\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 299 with batch size 971 and train loss 0.012324503622949123\n",
      "Finished epoch 299 with batch size 108 and validation loss 0.10295535624027252\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 300 with batch size 971 and train loss 0.013078057207167149\n",
      "Finished epoch 300 with batch size 108 and validation loss 0.10278595238924026\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 301 with batch size 971 and train loss 0.011763826943933964\n",
      "Finished epoch 301 with batch size 108 and validation loss 0.10264277458190918\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 302 with batch size 971 and train loss 0.012076426297426224\n",
      "Finished epoch 302 with batch size 108 and validation loss 0.10250605642795563\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 303 with batch size 971 and train loss 0.010926966555416584\n",
      "Finished epoch 303 with batch size 108 and validation loss 0.10232610255479813\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 304 with batch size 971 and train loss 0.012037081643939018\n",
      "Finished epoch 304 with batch size 108 and validation loss 0.10219178348779678\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 305 with batch size 971 and train loss 0.010978764854371548\n",
      "Finished epoch 305 with batch size 108 and validation loss 0.10201842337846756\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 306 with batch size 971 and train loss 0.011449778452515602\n",
      "Finished epoch 306 with batch size 108 and validation loss 0.10183195769786835\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 307 with batch size 971 and train loss 0.011840609833598137\n",
      "Finished epoch 307 with batch size 108 and validation loss 0.10168561339378357\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 308 with batch size 971 and train loss 0.011728297919034958\n",
      "Finished epoch 308 with batch size 108 and validation loss 0.10154314339160919\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 309 with batch size 971 and train loss 0.011709230951964855\n",
      "Finished epoch 309 with batch size 108 and validation loss 0.1013658195734024\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 310 with batch size 971 and train loss 0.01233588345348835\n",
      "Finished epoch 310 with batch size 108 and validation loss 0.10122767835855484\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 311 with batch size 971 and train loss 0.012711748480796814\n",
      "Finished epoch 311 with batch size 108 and validation loss 0.1010977253317833\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 312 with batch size 971 and train loss 0.011761391535401344\n",
      "Finished epoch 312 with batch size 108 and validation loss 0.10089368373155594\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 313 with batch size 971 and train loss 0.011358113959431648\n",
      "Finished epoch 313 with batch size 108 and validation loss 0.10073579847812653\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 314 with batch size 971 and train loss 0.01168436836451292\n",
      "Finished epoch 314 with batch size 108 and validation loss 0.10059083998203278\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 315 with batch size 971 and train loss 0.011759855784475803\n",
      "Finished epoch 315 with batch size 108 and validation loss 0.10049181431531906\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 316 with batch size 971 and train loss 0.010847167111933231\n",
      "Finished epoch 316 with batch size 108 and validation loss 0.10045447945594788\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 317 with batch size 971 and train loss 0.011225377209484577\n",
      "Finished epoch 317 with batch size 108 and validation loss 0.10052469372749329\n",
      "Finished epoch 318 with batch size 971 and train loss 0.010505924932658672\n",
      "Finished epoch 318 with batch size 108 and validation loss 0.10061467438936234\n",
      "Finished epoch 319 with batch size 971 and train loss 0.011743217706680298\n",
      "Finished epoch 319 with batch size 108 and validation loss 0.10077838599681854\n",
      "Finished epoch 320 with batch size 971 and train loss 0.011081132106482983\n",
      "Finished epoch 320 with batch size 108 and validation loss 0.10085539519786835\n",
      "Finished epoch 321 with batch size 971 and train loss 0.011178720742464066\n",
      "Finished epoch 321 with batch size 108 and validation loss 0.1008719727396965\n",
      "Finished epoch 322 with batch size 971 and train loss 0.011407704092562199\n",
      "Finished epoch 322 with batch size 108 and validation loss 0.1009189784526825\n",
      "Finished epoch 323 with batch size 971 and train loss 0.010004295967519283\n",
      "Finished epoch 323 with batch size 108 and validation loss 0.1008978933095932\n",
      "Finished epoch 324 with batch size 971 and train loss 0.01093403622508049\n",
      "Finished epoch 324 with batch size 108 and validation loss 0.10084801912307739\n",
      "Finished epoch 325 with batch size 971 and train loss 0.011649548076093197\n",
      "Finished epoch 325 with batch size 108 and validation loss 0.10071572661399841\n",
      "Finished epoch 326 with batch size 971 and train loss 0.009655729867517948\n",
      "Finished epoch 326 with batch size 108 and validation loss 0.10062211006879807\n",
      "Finished epoch 327 with batch size 971 and train loss 0.01096273772418499\n",
      "Finished epoch 327 with batch size 108 and validation loss 0.10045696794986725\n",
      "Finished epoch 328 with batch size 971 and train loss 0.009901429526507854\n",
      "Finished epoch 328 with batch size 108 and validation loss 0.10021558403968811\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 329 with batch size 971 and train loss 0.00993790477514267\n",
      "Finished epoch 329 with batch size 108 and validation loss 0.10000237077474594\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 330 with batch size 971 and train loss 0.01015169732272625\n",
      "Finished epoch 330 with batch size 108 and validation loss 0.09974484890699387\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 331 with batch size 971 and train loss 0.009540701285004616\n",
      "Finished epoch 331 with batch size 108 and validation loss 0.09945975244045258\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 332 with batch size 971 and train loss 0.01073674950748682\n",
      "Finished epoch 332 with batch size 108 and validation loss 0.09904123097658157\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 333 with batch size 971 and train loss 0.010211644694209099\n",
      "Finished epoch 333 with batch size 108 and validation loss 0.0986563190817833\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 334 with batch size 971 and train loss 0.009519926272332668\n",
      "Finished epoch 334 with batch size 108 and validation loss 0.09838508069515228\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 335 with batch size 971 and train loss 0.010965147987008095\n",
      "Finished epoch 335 with batch size 108 and validation loss 0.09805084764957428\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 336 with batch size 971 and train loss 0.010741743259131908\n",
      "Finished epoch 336 with batch size 108 and validation loss 0.09772004187107086\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 337 with batch size 971 and train loss 0.010141038335859776\n",
      "Finished epoch 337 with batch size 108 and validation loss 0.09743759036064148\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 338 with batch size 971 and train loss 0.009908471256494522\n",
      "Finished epoch 338 with batch size 108 and validation loss 0.09711945801973343\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 339 with batch size 971 and train loss 0.00909658707678318\n",
      "Finished epoch 339 with batch size 108 and validation loss 0.09687619656324387\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 340 with batch size 971 and train loss 0.009003751911222935\n",
      "Finished epoch 340 with batch size 108 and validation loss 0.09662895649671555\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 341 with batch size 971 and train loss 0.01050155982375145\n",
      "Finished epoch 341 with batch size 108 and validation loss 0.09639299660921097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the lowest validation loss so far.\n",
      "Finished epoch 342 with batch size 971 and train loss 0.008634592406451702\n",
      "Finished epoch 342 with batch size 108 and validation loss 0.09623266011476517\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 343 with batch size 971 and train loss 0.00873690564185381\n",
      "Finished epoch 343 with batch size 108 and validation loss 0.09610921144485474\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 344 with batch size 971 and train loss 0.009210296906530857\n",
      "Finished epoch 344 with batch size 108 and validation loss 0.09599277377128601\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 345 with batch size 971 and train loss 0.009608902968466282\n",
      "Finished epoch 345 with batch size 108 and validation loss 0.09594132006168365\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 346 with batch size 971 and train loss 0.009917051531374454\n",
      "Finished epoch 346 with batch size 108 and validation loss 0.09583486616611481\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 347 with batch size 971 and train loss 0.009794080629944801\n",
      "Finished epoch 347 with batch size 108 and validation loss 0.09571892023086548\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 348 with batch size 971 and train loss 0.01036473736166954\n",
      "Finished epoch 348 with batch size 108 and validation loss 0.09553276747465134\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 349 with batch size 971 and train loss 0.009487350471317768\n",
      "Finished epoch 349 with batch size 108 and validation loss 0.09540442377328873\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 350 with batch size 971 and train loss 0.00926089659333229\n",
      "Finished epoch 350 with batch size 108 and validation loss 0.09534712880849838\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 351 with batch size 971 and train loss 0.009070588275790215\n",
      "Finished epoch 351 with batch size 108 and validation loss 0.09534556418657303\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 352 with batch size 971 and train loss 0.008328362368047237\n",
      "Finished epoch 352 with batch size 108 and validation loss 0.09530830383300781\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 353 with batch size 971 and train loss 0.008663255721330643\n",
      "Finished epoch 353 with batch size 108 and validation loss 0.09519382566213608\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 354 with batch size 971 and train loss 0.00927533395588398\n",
      "Finished epoch 354 with batch size 108 and validation loss 0.09509783983230591\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 355 with batch size 971 and train loss 0.008749081753194332\n",
      "Finished epoch 355 with batch size 108 and validation loss 0.09499935805797577\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 356 with batch size 971 and train loss 0.008715837262570858\n",
      "Finished epoch 356 with batch size 108 and validation loss 0.09497658163309097\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 357 with batch size 971 and train loss 0.008873149752616882\n",
      "Finished epoch 357 with batch size 108 and validation loss 0.09500359743833542\n",
      "Finished epoch 358 with batch size 971 and train loss 0.008932341821491718\n",
      "Finished epoch 358 with batch size 108 and validation loss 0.09502892196178436\n",
      "Finished epoch 359 with batch size 971 and train loss 0.008308589458465576\n",
      "Finished epoch 359 with batch size 108 and validation loss 0.09506729245185852\n",
      "Finished epoch 360 with batch size 971 and train loss 0.008542262949049473\n",
      "Finished epoch 360 with batch size 108 and validation loss 0.09508534520864487\n",
      "Finished epoch 361 with batch size 971 and train loss 0.009357580915093422\n",
      "Finished epoch 361 with batch size 108 and validation loss 0.0950138047337532\n",
      "Finished epoch 362 with batch size 971 and train loss 0.009046702645719051\n",
      "Finished epoch 362 with batch size 108 and validation loss 0.09494424611330032\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 363 with batch size 971 and train loss 0.008828043937683105\n",
      "Finished epoch 363 with batch size 108 and validation loss 0.09488830715417862\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 364 with batch size 971 and train loss 0.00798733253031969\n",
      "Finished epoch 364 with batch size 108 and validation loss 0.09490915387868881\n",
      "Finished epoch 365 with batch size 971 and train loss 0.00811468344181776\n",
      "Finished epoch 365 with batch size 108 and validation loss 0.09484264254570007\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 366 with batch size 971 and train loss 0.008007721975445747\n",
      "Finished epoch 366 with batch size 108 and validation loss 0.09483984857797623\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 367 with batch size 971 and train loss 0.007825640961527824\n",
      "Finished epoch 367 with batch size 108 and validation loss 0.09475380927324295\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 368 with batch size 971 and train loss 0.00904990267008543\n",
      "Finished epoch 368 with batch size 108 and validation loss 0.09466123580932617\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 369 with batch size 971 and train loss 0.008357376791536808\n",
      "Finished epoch 369 with batch size 108 and validation loss 0.09466186165809631\n",
      "Finished epoch 370 with batch size 971 and train loss 0.008127369917929173\n",
      "Finished epoch 370 with batch size 108 and validation loss 0.09461932629346848\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 371 with batch size 971 and train loss 0.008106902241706848\n",
      "Finished epoch 371 with batch size 108 and validation loss 0.09455090016126633\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 372 with batch size 971 and train loss 0.009044211357831955\n",
      "Finished epoch 372 with batch size 108 and validation loss 0.09450136125087738\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 373 with batch size 971 and train loss 0.00862613320350647\n",
      "Finished epoch 373 with batch size 108 and validation loss 0.09444092214107513\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 374 with batch size 971 and train loss 0.008151496760547161\n",
      "Finished epoch 374 with batch size 108 and validation loss 0.09428619593381882\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 375 with batch size 971 and train loss 0.00837790034711361\n",
      "Finished epoch 375 with batch size 108 and validation loss 0.09405652433633804\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 376 with batch size 971 and train loss 0.007734408602118492\n",
      "Finished epoch 376 with batch size 108 and validation loss 0.09383133798837662\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 377 with batch size 971 and train loss 0.007705239113420248\n",
      "Finished epoch 377 with batch size 108 and validation loss 0.09358888119459152\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 378 with batch size 971 and train loss 0.0075967456214129925\n",
      "Finished epoch 378 with batch size 108 and validation loss 0.0933915302157402\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 379 with batch size 971 and train loss 0.008345394395291805\n",
      "Finished epoch 379 with batch size 108 and validation loss 0.09321141988039017\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 380 with batch size 971 and train loss 0.007749433629214764\n",
      "Finished epoch 380 with batch size 108 and validation loss 0.09289079904556274\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 381 with batch size 971 and train loss 0.008123750798404217\n",
      "Finished epoch 381 with batch size 108 and validation loss 0.09265178442001343\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 382 with batch size 971 and train loss 0.007702564354985952\n",
      "Finished epoch 382 with batch size 108 and validation loss 0.09245491027832031\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 383 with batch size 971 and train loss 0.007223788183182478\n",
      "Finished epoch 383 with batch size 108 and validation loss 0.0922560840845108\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 384 with batch size 971 and train loss 0.0071966927498579025\n",
      "Finished epoch 384 with batch size 108 and validation loss 0.09206139296293259\n",
      "This is the lowest validation loss so far.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 385 with batch size 971 and train loss 0.00752194644883275\n",
      "Finished epoch 385 with batch size 108 and validation loss 0.0918937399983406\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 386 with batch size 971 and train loss 0.00790584646165371\n",
      "Finished epoch 386 with batch size 108 and validation loss 0.09171172976493835\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 387 with batch size 971 and train loss 0.008349933661520481\n",
      "Finished epoch 387 with batch size 108 and validation loss 0.09162647277116776\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 388 with batch size 971 and train loss 0.007234587334096432\n",
      "Finished epoch 388 with batch size 108 and validation loss 0.09154155850410461\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 389 with batch size 971 and train loss 0.007024056743830442\n",
      "Finished epoch 389 with batch size 108 and validation loss 0.09142237901687622\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 390 with batch size 971 and train loss 0.008390475064516068\n",
      "Finished epoch 390 with batch size 108 and validation loss 0.09134214371442795\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 391 with batch size 971 and train loss 0.007663299795240164\n",
      "Finished epoch 391 with batch size 108 and validation loss 0.09119979292154312\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 392 with batch size 971 and train loss 0.007247991394251585\n",
      "Finished epoch 392 with batch size 108 and validation loss 0.09105802327394485\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 393 with batch size 971 and train loss 0.006296584848314524\n",
      "Finished epoch 393 with batch size 108 and validation loss 0.09089988470077515\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 394 with batch size 971 and train loss 0.007617468014359474\n",
      "Finished epoch 394 with batch size 108 and validation loss 0.09073425829410553\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 395 with batch size 971 and train loss 0.0074641830287873745\n",
      "Finished epoch 395 with batch size 108 and validation loss 0.09049297869205475\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 396 with batch size 971 and train loss 0.007213704288005829\n",
      "Finished epoch 396 with batch size 108 and validation loss 0.09031954407691956\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 397 with batch size 971 and train loss 0.006356707774102688\n",
      "Finished epoch 397 with batch size 108 and validation loss 0.09025406837463379\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 398 with batch size 971 and train loss 0.007396235596388578\n",
      "Finished epoch 398 with batch size 108 and validation loss 0.09024813771247864\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 399 with batch size 971 and train loss 0.006590188946574926\n",
      "Finished epoch 399 with batch size 108 and validation loss 0.09020259976387024\n",
      "This is the lowest validation loss so far.\n",
      "Finished epoch 400 with batch size 971 and train loss 0.006832418963313103\n",
      "Finished epoch 400 with batch size 108 and validation loss 0.09025834500789642\n",
      "Finished epoch 401 with batch size 971 and train loss 0.006999385543167591\n",
      "Finished epoch 401 with batch size 108 and validation loss 0.09036827832460403\n",
      "Finished epoch 402 with batch size 971 and train loss 0.006175465416163206\n",
      "Finished epoch 402 with batch size 108 and validation loss 0.09048372507095337\n",
      "Finished epoch 403 with batch size 971 and train loss 0.006179295480251312\n",
      "Finished epoch 403 with batch size 108 and validation loss 0.0905858650803566\n",
      "Finished epoch 404 with batch size 971 and train loss 0.0074136215262115\n",
      "Finished epoch 404 with batch size 108 and validation loss 0.09064794331789017\n",
      "Finished epoch 405 with batch size 971 and train loss 0.007447484415024519\n",
      "Finished epoch 405 with batch size 108 and validation loss 0.09067434817552567\n",
      "Finished epoch 406 with batch size 971 and train loss 0.007466916460543871\n",
      "Finished epoch 406 with batch size 108 and validation loss 0.09064915776252747\n",
      "Finished epoch 407 with batch size 971 and train loss 0.0071494560688734055\n",
      "Finished epoch 407 with batch size 108 and validation loss 0.09056423604488373\n",
      "Finished epoch 408 with batch size 971 and train loss 0.007926632650196552\n",
      "Finished epoch 408 with batch size 108 and validation loss 0.0905805304646492\n",
      "Finished epoch 409 with batch size 971 and train loss 0.007951474748551846\n",
      "Finished epoch 409 with batch size 108 and validation loss 0.0905427560210228\n",
      "Finished epoch 410 with batch size 971 and train loss 0.006480688229203224\n",
      "Finished epoch 410 with batch size 108 and validation loss 0.09050579369068146\n",
      "Finished epoch 411 with batch size 971 and train loss 0.006987941917032003\n",
      "Finished epoch 411 with batch size 108 and validation loss 0.09043234586715698\n",
      "Finished epoch 412 with batch size 971 and train loss 0.007010994013398886\n",
      "Finished epoch 412 with batch size 108 and validation loss 0.09032095223665237\n",
      "Finished epoch 413 with batch size 971 and train loss 0.007585177663713694\n",
      "Finished epoch 413 with batch size 108 and validation loss 0.090274877846241\n",
      "Finished epoch 414 with batch size 971 and train loss 0.006429451052099466\n",
      "Finished epoch 414 with batch size 108 and validation loss 0.09028888493776321\n",
      "Finished epoch 415 with batch size 971 and train loss 0.007239670492708683\n",
      "Finished epoch 415 with batch size 108 and validation loss 0.09032092243432999\n",
      "Finished epoch 416 with batch size 971 and train loss 0.006658227648586035\n",
      "Finished epoch 416 with batch size 108 and validation loss 0.09032918512821198\n",
      "Finished epoch 417 with batch size 971 and train loss 0.006735513918101788\n",
      "Finished epoch 417 with batch size 108 and validation loss 0.09031259268522263\n",
      "Finished epoch 418 with batch size 971 and train loss 0.006669685710221529\n",
      "Finished epoch 418 with batch size 108 and validation loss 0.090221107006073\n",
      "Finished epoch 419 with batch size 971 and train loss 0.006694846786558628\n",
      "Finished epoch 419 with batch size 108 and validation loss 0.09020572900772095\n",
      "Validation loss did not improve after 20 tries. Stopping\n",
      "Elapsed time: 96.20874381065369\n",
      "Score: 0.941317365269461\n"
     ]
    }
   ],
   "source": [
    "for parameter in comb_parameters:\n",
    "    model = NLS(\n",
    "        verbose=2\n",
    "        , es=True\n",
    "        , gpu=True\n",
    "        , scale_data=False\n",
    "        , varying_theta0=False\n",
    "        , fixed_theta0=True\n",
    "        , dataloader_workers=0\n",
    "        # , with_mean=False\n",
    "        , **parameter\n",
    "    )\n",
    "\n",
    "    train_vectors = train_vectors.toarray()\n",
    "    model.fit(x_train=train_vectors, y_train=newsgroups_train.target)\n",
    "\n",
    "    test_vectors = test_vectors.toarray()\n",
    "    pred = model.predict(test_vectors)\n",
    "\n",
    "    # pickle.dump(model, open(\"model.pickle\", \"wb\"))\n",
    "    # print(newsgroups_test.target)\n",
    "    # print(pred)\n",
    "\n",
    "    print('Score:', sklearn.metrics.f1_score(newsgroups_test.target, pred, average='binary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document id: 83\n",
      "[[0]]\n",
      "Probability(christian) = [[0.5356064  0.46439368]]\n",
      "True class: atheism\n",
      "True class number: 0\n",
      "Text:\n",
      "From: johnchad@triton.unm.edu (jchadwic)\n",
      "Subject: Another request for Darwin Fish\n",
      "Organization: University of New Mexico, Albuquerque\n",
      "Lines: 11\n",
      "NNTP-Posting-Host: triton.unm.edu\n",
      "\n",
      "Hello Gang,\n",
      "\n",
      "There have been some notes recently asking where to obtain the DARWIN fish.\n",
      "This is the same question I have and I have not seen an answer on the\n",
      "net. If anyone has a contact please post on the net or email me.\n",
      "\n",
      "Thanks,\n",
      "\n",
      "john chadwick\n",
      "johnchad@triton.unm.edu\n",
      "or\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = 83\n",
    "newsgroups_test.data[idx]\n",
    "\n",
    "print('Document id: %d' % idx)\n",
    "# print(model.predict(test_vectors))\n",
    "print(model.predict([test_vectors[idx]]))\n",
    "print('Probability(christian) =', model.predict_proba([test_vectors[idx]]))\n",
    "\n",
    "print('True class: %s' % class_names[newsgroups_test.target[idx]])\n",
    "print('True class number:',newsgroups_test.target[idx] )\n",
    "print('Text:')\n",
    "print(newsgroups_test.data[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_explain = [test_vectors[idx]]\n",
    "explanation = model.get_thetas(x_pred=x_explain, net_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = explanation[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00185837,  0.03762029,  0.00095214, ...,  0.01973594,\n",
       "       -0.00247948,  0.03433306], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 23035, 2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_0_abs = betas[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00185837,  0.03762029,  0.00095214, ...,  0.01973594,\n",
       "       -0.00247948,  0.03433306], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_0_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_words = np.flip(np.argsort(beta_0_abs))[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06623404"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(beta_0_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06623404, 0.06347304, 0.06247774, 0.06198891, 0.06145162,\n",
       "       0.06080218, 0.06078856, 0.06075222, 0.06006645, 0.05940148,\n",
       "       0.05909632, 0.05895659, 0.05888094, 0.05885819, 0.05837996,\n",
       "       0.05809595, 0.05718294, 0.05714855, 0.05674302, 0.05667946,\n",
       "       0.05657919, 0.05633802, 0.05616375, 0.05605442, 0.055861  ,\n",
       "       0.05562407, 0.05557979, 0.05549935, 0.0554196 , 0.05526735,\n",
       "       0.05517265, 0.05494512, 0.05487379, 0.05448806, 0.05447131,\n",
       "       0.05444276, 0.0543652 , 0.05424865, 0.05423601, 0.05416946,\n",
       "       0.05412706, 0.05412158, 0.05405178, 0.0539417 , 0.05389514,\n",
       "       0.05373982, 0.05362725, 0.05351402, 0.05346209, 0.05330558,\n",
       "       0.05313684, 0.05302996, 0.05288646, 0.05269662, 0.05264094,\n",
       "       0.05239429, 0.05238328, 0.05223578, 0.05211793, 0.0518931 ,\n",
       "       0.05178832, 0.05167955, 0.05162209, 0.05162086, 0.05158719,\n",
       "       0.05157619, 0.05150229, 0.05138344, 0.05124108, 0.05121644,\n",
       "       0.05120005, 0.05118139, 0.05116572, 0.05109273, 0.05099122,\n",
       "       0.05095812, 0.05090057, 0.05079318, 0.05073821, 0.05073512,\n",
       "       0.0505904 , 0.05056096, 0.05054268, 0.0505259 , 0.0504364 ,\n",
       "       0.05041678, 0.0503768 , 0.05030306, 0.05014986, 0.05012479,\n",
       "       0.05011612, 0.05004323, 0.05003676, 0.05002538, 0.05000455,\n",
       "       0.04996108, 0.0497134 , 0.0496989 , 0.04948711, 0.04947325],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_0_abs[important_words]a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Resident', 'Posting', 'SOME', 'versed', 'NoSubdomain', 'Penalty',\n",
       "       'mathew', 'NoDomain', 'CSNet', 'ttu', 'claws', 'tektronix',\n",
       "       'Philistines', 'Seattle', 'supercooled', 'Nice', 'cooperative',\n",
       "       'Bill', 'unselfishness', 'temperatures', 'Mongols', '1971',\n",
       "       'JEWISH', 'SSAUYET', 'relaxing', 'Okcforum', 'MUST', 'Centigram',\n",
       "       'dbstu1', 'Anyways', 'tclark', 'supercede', 'Ryan', 'Host', 'bil',\n",
       "       'Quran', 'finals', 'Conner', 'map', 'mangoe', 'jmunch', 'Manson',\n",
       "       'Bangladesh', 'LAW', 'Germany', '2944159064', 'Uk', 'Stilgar',\n",
       "       'Wilkins', 'Consultants', 'biological', 'dollars', 'TekLabs',\n",
       "       'newton', 'elee', 'ALink', 'UNC', 'sail', 'courtroom', 'atrocious',\n",
       "       'kmr4', 'VMS', 'Nntp', 'Cornflakes', 'madhaus', 'extermination',\n",
       "       'syllogism', 'ZEUS', 'FINABO', 'slick', 'Strom', '930405',\n",
       "       'invention', 'CWRU', 'Next', 'andrew', 'Wake', 'Skeptic', 'antioc',\n",
       "       '2000', 'red', 'Cranford', 'Inc', 'shrike', 'stopped', 'mantis',\n",
       "       'crchh410', 'Distribution', 'snm6394', 'failed', 'flibble',\n",
       "       'Technical', 'Subsequently', 'funds', 'Highness', 'equated',\n",
       "       'VNEWS', 'awesome', 'dolphins', 'CAC'], dtype='<U80')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_features[important_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "From: johnchad@triton.unm.edu (jchadwic)\n",
      "Subject: Another request for Darwin Fish\n",
      "Organization: University of New Mexico, Albuquerque\n",
      "Lines: 11\n",
      "NNTP-Posting-Host: triton.unm.edu\n",
      "\n",
      "Hello Gang,\n",
      "\n",
      "There have been some notes recently asking where to obtain the DARWIN fish.\n",
      "This is the same question I have and I have not seen an answer on the\n",
      "net. If anyone has a contact please post on the net or email me.\n",
      "\n",
      "Thanks,\n",
      "\n",
      "john chadwick\n",
      "johnchad@triton.unm.edu\n",
      "or\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Text:')\n",
    "print(newsgroups_test.data[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
